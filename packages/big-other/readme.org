#+title: Readme
Tools for quotes, names, things, books
* makefile
#+begin_src makefile :tangle Makefile

sources = \
	bo_texts/ij.txt \
	bo_texts/ta.txt \
	bo_texts/rm.txt \
	bo_texts/df.txt \
	bo_texts/bj.txt \
    bo_texts/dq.txt \
    bo_texts/fp.txt \
    bo_texts/ccru.txt \
    bo_texts/td.txt

bo_texts/%.txt: epub/%.epub
	pandoc -f epub -t plain -s -o $@ $<

books: $(sources)

.PHONY: clean
clean:
	rm -f $(sources)


# end
#+end_src
* quotes
#+name: quotes_sql
#+begin_src sqlite :db ./bigother.db :var book='df' limit=10 cluster=1
SELECT replace(json_extract(quotes.data, '$.text'), CHAR(10), ' ')
                       from quotes
                       inner join clusters
                             on quotes.id = clusters.id
                       where book = '$book' and
                       clusters.cluster = $cluster
                       order by random() limit $limit;
#+end_src

#+RESULTS: quotes_sql
| â€œAsk for leave.â€                                                                                          |
| â€œItâ€™s a nice place, and easy to get to by bus. Lately, Iâ€™ve been coming here to take walks fairly often.â€ |
| â€œDo they really believe the sophon will unfold in lower dimensions?â€                                      |
| â€œI donâ€™t like modern art.â€                                                                                |
| â€œI wonâ€™t be seeing them, either,â€                                                                         |
| â€œInfiltrate his kitchen!â€                                                                                 |
| â€œThe probeâ€™s gone out!â€                                                                                   |
| â€œHmm. Maybe,â€                                                                                             |
| â€œGet going?â€                                                                                              |
| â€œBut you canâ€™t simply not care. Humanity will be gone in four hundred years!â€                             |


#+name: quotes
#+begin_src python :var q=quotes_sql()
return list(map(lambda a: a[0],q))
#+end_src

#+RESULTS: quotes
| â€œWho? The assailant?â€ | â€œMr. Hines, your plan is like some sort of naÃ¯ve daydream.â€ | â€œCharge. Recharge,â€ | â€œWho is reliable?â€ | â€œHowever, Dr. Luo, I have a small piece of advice,â€ | â€œNo matter how many steps back you take, thought control is absolutely unacceptable,â€ | â€œI just feel for the kids,â€ | â€œOne exception is sufficient.â€ | â€œVery well. Iâ€™ll go to the city, then.â€ | â€œIs there any need for me to continue?â€ |

* sentences

#+name: get_sentence_json
#+begin_src sqlite :db ./bigother.db :var id=422
SELECT data FROM sentences where sentences.id=$id;
#+end_src


#+name: get_sentence
#+begin_src python :var id=48000 q="SELECT data FROM sentences where sentences.id=?"
import sqlite3
import codecs

# connect to the database
conn = sqlite3.connect('bigother.db')
cursor = conn.cursor()
def detokenize(sentence):
    text = ''
    for t in sentence["tokens"]:
        text += t["originalText"] + t["after"]
    return text

cursor.execute(q, (id,))
raw = cursor.fetchone()[0]
#parse json
import json
data = json.loads(raw)
st= detokenize(data)
st=st.replace('\n', ' ')
return st
#+end_src


#+name: get_random_sentence
#+call: get_sentence(1, "SELECT data FROM sentences order by random() limit ?")

#+call: get_random_sentence()

#+RESULTS:
: The ZaÅ‚uski Brothersâ€™ Library and Canon Benedykt Chmielowski  The collections amassed so industriously and at such extravagant expense by the two brother bishops, the state of which had so worried Father Chmielowski, eventually attained an unprecedented sizeâ€”around four hundred thousand volumes and twenty thousand manuscripts, not even c

* names
#+name: create_name_view
#+begin_src sqlite :db ./bigother.db
drop table namess;
create table namess (
        id integer PRIMARY KEY AUTOINCREMENT,
        n text,
        book text
        );
INSERT INTO namess (n,book)
WITH people as (
SELECT x.value->'text' as t,
json_extract(x.value, '$.ner') as ner,
book FROM sentences,
json_each(sentences.data->'entitymentions') as x
where ner = 'PERSON'
and not lower(t) like '%his%'
and not lower(t) like '%hers%'
and not lower(t) like '%him%'
and not lower(t) like '%her%'
and not lower(t) like '%he%'
and not lower(t) like '%she%'
) select replace(t,CHAR(10), ' ') as nname, book from people;
#+end_src

#+RESULTS: create_name_view


#+name: fix-names
#+begin_src python :output file :file fix-names
import sqlite3
import codecs

# connect to the database
conn = sqlite3.connect('bigother.db')
cursor = conn.cursor()

# select the column and update the values
l =[]
for row in cursor.execute("SELECT id,n FROM namess"):
    print(row[1])
    old_value = row[1]
    # remove new lines and unicode characters
    new_value = old_value.encode('unicode_escape').decode('unicode_escape')
    new_value = new_value.replace('\\n', ' ').replace('\\r', ' ')
    # new_value = new_value.replace('\u[a-fA-F0-9]{4}', "")
    l.append((new_value, row[0]))

cursor.executemany("UPDATE namess SET n=? WHERE id=?", l)
# commit the changes and close the connection
conn.commit()
conn.close()
#+end_src


#+name: count_names
#+begin_src sqlite  :db ./bigother.db
with a as (select distinct n from namess)
       select count(n) from a;
#+end_src

#+RESULTS: count_names
: 7295

#+name: random_names2
#+begin_src sqlite  :db ./bigother.db :var limit=10
select distinct n from namess where n regexp '\w+\s+\w+' order by random() limit $limit;
#+end_src


#+name: random_names
#+begin_src python :var limit=10
import sqlite3
# connect to the database
conn = sqlite3.connect('bigother.db')
cursor = conn.cursor()
# select the column and update the values
cursor.execute("SELECT DISTINCT n FROM namess ORDER BY RANDOM() LIMIT ?;", (limit,))
l=cursor.fetchall()

fixed=map(lambda x: (x[0].encode('unicode_escape').decode('unicode_escape'),), l)
return list(fixed)
#+end_src

#+RESULTS: random_names
| "Zhang Beihai"          |
| "Marlowe"               |
| "Leopold M\u2019Intosh" |
| "Hokusai"               |
| "Mongo"                 |
| "Louis J\nWalsh"        |
| "Flipperty Jippert"     |
| "Charles Tavises"       |
| "Xanthippe"             |
| "Amparo"                |

#+name: random_two_names
#+begin_src python :var l=random_names2(2) :results string
fixed=map(lambda x: (x[0].encode('unicode_escape').decode('unicode_escape'),), l)
return list(fixed)
#+end_src

#+RESULTS: random_two_names
| Martin Harvey    |
| Rualdus Columbus |
* is offensive?
#+name: is_offensive_lib
#+begin_src python :var m="offensive"
from transformers import AutoModelForSequenceClassification
from transformers import TFAutoModelForSequenceClassification
from transformers import AutoTokenizer
import numpy as np
from scipy.special import softmax
import csv
import urllib.request

# Preprocess text (username and link placeholders)
def preprocess(text):
    new_text = []
    for t in text.split(" "):
        t = '@user' if t.startswith('@') and len(t) > 1 else t
        t = 'http' if t.startswith('http') else t
        new_text.append(t)
    return " ".join(new_text)

# Tasks:
# emoji, emotion, hate, irony, offensive, sentiment
# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary



def proc(t,tokenizer,model):
    text = preprocess(t)
    encoded_input = tokenizer(text, return_tensors='pt')
    output = model(**encoded_input)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    return scores
# # TF
# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)
# model.save_pretrained(MODEL)

# text = "Good night ÃƒÂ°Ã‚ÂŸÃ‚Â˜Ã‚ÂŠ"
# encoded_input = tokenizer(text, return_tensors='tf')
# output = model(encoded_input)
# scores = output[0][0].numpy()
# scores = softmax(scores)

def finalize(scores,t,task,labels):
    ranking = np.argsort(scores)
    ranking = ranking[::-1]
    output = [t]
    for i in range(scores.shape[0]):
        l = labels[ranking[i]]
        if(l==task):
            s = scores[ranking[i]]
            output.append((l, s))
    return output

def init(task):
    MODEL = f"twitter-roberta-base-{task}"
    tokenizer = AutoTokenizer.from_pretrained(MODEL, local_files_only=False)
    # PT
    model = AutoModelForSequenceClassification.from_pretrained(MODEL)
    model.save_pretrained(MODEL)
    # download label mapping
    labels=[]
    mapping_link = f"{task}-mapping.txt"
    with open(mapping_link, "rb") as f:
        html = f.read().decode('utf-8').split("\n")
        csvreader = csv.reader(html, delimiter='\t')
        labels = [row[1] for row in csvreader if len(row) > 1]
    return tokenizer,model,labels
#+end_src


#+name: is_offensive
#+begin_src python :var t=quotes() m="offensive" :noweb yes
<<is_offensive_lib>>
task=m
tokenizer,model,labels = init(task)
scores = [proc(text,tokenizer,model) for text in t]
output = [finalize(d,t[i],task,labels) for i,d in enumerate(scores)]
return output
#+end_src

#+RESULTS: is_offensive
| â€œBecause of Belphegor, Phantom of the Louvre, right? Sophie Marceau is gorgeous. Sheâ€™s got Eastern looks, too.â€                                                        | (offensive 0.057934664) |
| â€œStupid children. Run!â€                                                                                                                                                | (offensive 0.83124995)  |
| â€œI am become death, the destroyer of worlds,â€                                                                                                                          | (offensive 0.44300583)  |
| â€œEverything you saw was the real her. Everything you knew about her was true. Everything that made her her: Her past life, her family, her personality, and her mind.â€ | (offensive 0.052133456) |
| â€œIn every direction.â€                                                                                                                                                  | (offensive 0.14158154)  |
| â€œIf I canâ€™t send a spell out into the universe, thereâ€™s nothing I can do.â€                                                                                             | (offensive 0.12453921)  |
| â€œIf thatâ€™s true, then thereâ€™ll be more comrades gathering here next time. Good-bye.â€                                                                                   | (offensive 0.057362314) |
| â€œMeteor shower!â€                                                                                                                                                       | (offensive 0.18150623)  |
| â€œBack home. Iâ€™m getting ready for hibernation.â€                                                                                                                        | (offensive 0.15959275)  |
| â€œFrom the moment I became a soldier, I was prepared to go there if necessary,â€                                                                                         | (offensive 0.089419656) |
* processing script
#+begin_src python :tangle process.py :noeval yes
#! /usr/bin/env nix-shell
#! nix-shell -i python3 -p python3Packages.stanza -p glibc
from stanza.server import CoreNLPClient
import json
import hashlib
import sqlite3
from multiprocessing import Pool, cpu_count, Lock
import sys
import os
import time
import stanza


mutex = Lock()
# connect to the database
conn = sqlite3.connect('bigother.db', check_same_thread=False)

# create a cursor object
cursor = conn.cursor()

max_chars = 10000
client = CoreNLPClient(
            endpoint='http://yui:9000',
            output_format='json',
            start_server="false",
            max_char_length=max_chars,
            timeout=90000)


def process(text):
    ann = client.annotate(text)
    return ann
    # for q in ann.quote:
    #     print(q.text)
    # for m in ann.mentions:
    #     print(m.entityMentionText, m.entityType)

# clump large amounts of text into smaller chunks, based on a max character count.
# This is done character by character, so it doesn't assume new lines.
# But allow overlap to not split sentences.
def clump(text, max_chars=10000, overlap=200):
    chunks = []
    chunk = ''
    for char in text:
        chunk += char
        if len(chunk) >= max_chars:
            chunks.append(chunk)
            chunk = chunk[-overlap:]
    chunks.append(chunk)
    return chunks

def detokenize(sentence):
    text = ''
    for t in sentence["tokens"]:
        text += t["originalText"] + t["after"]
    return text

def hash_content(content):
    hash_object = hashlib.sha256(content.encode())
    hex_dig = hash_object.hexdigest()
    return hex_dig

def save_to_db(ann, book):
    cursor.execute('''CREATE TABLE IF NOT EXISTS sentences
                    (id INTEGER PRIMARY KEY,
                     book TEXT,
                     data TEXT)''')
    cursor.execute('''CREATE TABLE IF NOT EXISTS quotes
                    (id INTEGER PRIMARY KEY,
                     hash TEXT UNIQUE,
                     book TEXT,
                     data TEXT)''')
    for s in ann['sentences']:
        cursor.execute('''INSERT OR REPLACE INTO sentences (book,data) VALUES (?, ?)''',
                       (book, json.dumps(s)))
    for q in ann['quotes']:
        cursor.execute('''INSERT OR REPLACE INTO quotes (book,hash,data) VALUES (?, ?, ?)''',
                       (book, hash_content(q['text']), json.dumps(q)))
    conn.commit()

def process_chunk(args):
    chunk, book = args
    ann = process(chunk)
    mutex.acquire()
    save_to_db(ann, book)
    mutex.release()


# chunk stdin into smaller chunks, process each chunk, and print the results
def proc(filepath, name):
    with open(filepath) as f:
        text = f.read()
        chunks = clump(text, max_chars=max_chars)

        # create a pool of worker processes
        pool = Pool(processes=cpu_count()-5)

        # process each chunk concurrently
        args_list = [(chunk, name) for chunk in chunks]
        result = pool.map_async(process_chunk, args_list, chunksize=1)

        while not result.ready():
            # print progress information while waiting for the workers to finish
            processed = len(chunks) - result._number_left
            print(f"Processed {processed} of {len(chunks)} chunks of {name}.  Number left: {result._number_left}")
            time.sleep(1)

def process_files(files):
    for f in files:
        name = os.path.splitext(os.path.basename(f))[0]
        proc(f,name)


if __name__ == '__main__':
    # print(__name__)
    # files=["bj.txt", "df.txt", "dq.txt", "em.txt", "fp.txt", "ij.txt", "mb.txt", "rm.txt", "ta.txt", "td.txt", "u.txt"]
    files=[sys.argv[1]]
    process_files(files)
    conn.close()
#+end_src
* epub2txt
#+begin_src python :tangle epub2txt.py :noeval yes
import ebooklib
from ebooklib import epub
import sys

from bs4 import BeautifulSoup
# Open the epub file
book = epub.read_epub(sys.argv[1])

# Extract text from all chapters and concatenate into one variable
text = ''
for doc in book.get_items():
    print(doc.get_type())
    soup = BeautifulSoup(doc.get_content(),  features='lxml')
    text += soup.get_text()

#save the text to a file
with open(sys.argv[2], 'w') as f:
    f.write(text)
#+end_src
* srts
https://stackoverflow.com/a/55718903
#+begin_src python
import re
import pysrt

text = """
1
00:00:11,636 --> 00:00:13,221
Josh communicated but

2
00:00:13,221 --> 00:00:16,850
it's also the belief that
we never knew the severity
"""
srts=pysrt.from_string(text)


# regex = re.compile(r"\d+\n+[0-9\:,\-\>\s]{29}\n(.+|(\n[^\n]))+")
# raw_result = regex.findall(text)
# parsed_result = []
# for chunk in raw_result:
#     id, time, *lines = chunk.split("\n")
#     print(time)
#     ol=""
#     for l in lines:
#        ol +=l
#     parsed_result.append()
    # start, end = time.split(" --> ")
    # content = "\n".join(lines)
    # parsed_result.append({"id": id, "start": start, "end": end, "content": content})

return [s.text for s in srts]
#+end_src

#+RESULTS:
| Josh communicated but | it's also the belief that\nwe never knew the severity |


#+begin_src deno
const t = [1,23]
return t
#+end_src

#+RESULTS:
| 1 | 23 |
* tweet threads
#+name: threadd
#+begin_src sqlite :db bigother.db :var cid=29
with convo_l as (
     select distinct convo_id as cid from tweet_threads
        )
select tweets.tweet_text, author_id from tweet_threads join tweets on
tweets.tweet_id == tweet_threads.tweet_id
where convo_id = $cid order by position;
#+end_src

#+RESULTS: threadd
| @prolegomenatoy1 Greetings, this thread has been successfully registered and will be added to the joegame desert. Thank you! | 1465357441319776258 |
| @joegame_ ?                                                                                                    | 1295845511019278337 |
| @joegame_ test!                                                                                                | 1295845511019278337 |


#+name: thread
#+begin_src python :var t=threadd(567) :results value scalar
import re
def remove_twitter_handles(s):
    username_pattern = r'(?:\@[\w_]+ ?)' # regex pattern for Twitter handles
    url_pattern = r'http\S+'
    s= re.sub(username_pattern, '', s)
    s= re.sub(url_pattern, '', s)
    return s

def format_thread(s,author):
    return f'{remove_twitter_handles(s)} -- {author}'

return '\n'.join([format_thread(x[0],x[1]) for x in t])
#+end_src

#+RESULTS: thread
: Use of the word â€œbillionaireâ€ as a pejorative is morally wrong &amp; dumb ðŸ˜› -- 44196397
: "The average person doesn't even understand that billionaires don't actually have billions of dollars in the bank. --
:  --
: 99% of ""billionaire"" money is tied up in stocks/real estate." -- 2694154514
: Yet they get to borrow against as though theyâ€™re real assets.  Liquid assets.  Nice system we have.  For rich people. -- 1220747558659641344
: EVERY AMERICAN can BORROW against their assets. Every heard of taking alone against your house? Itâ€™s not a billionaire loophole, if you dont like the way the system works go out, campaign, get elected and try to change it. -- 1412881905226330115
: Oh ya poor people are famous for having assets to borrow money against -- 1519336873180631040
: Well technically they can take something to the pawn shop and get some money against something they own then get it back later. Same thing -- 1412881905226330115


#+name: thread
* bulk tweets

#+begin_src sqlite :db bigother.db :var cid=29
drop table bulk_tweets;
create table bulk_tweets (id INTEGER PRIMARY KEY, tweet_id INTEGER unique,
             FOREIGN KEY (tweet_id) REFERENCES tweets (tweet_id));
insert into bulk_tweets (tweet_id) select tweet_id from tweet_threads where convo_id > 2680;
#+end_src

#+RESULTS:
* cluster
#+cluster
#+begin_src python
#! /usr/bin/env nix-shell
#! nix-shell -i python3 -p python3Packages.scikit-learn
import sys
import sqlite3
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# Connect to the SQLite database containing the sentences
conn = sqlite3.connect('bigother.db')
c = conn.cursor()

c.execute('''CREATE TABLE IF NOT EXISTS clusters
                (id INTEGER PRIMARY KEY,
                 cluster INTEGER,
                 FOREIGN KEY (id) REFERENCES quotes(id))''')


# Retrieve the sentences and their IDs from the database

def run(book):
    sentences = []
    ids = []
    for row in c.execute('SELECT id, json_extract(data, \'$.text\') AS quote FROM quotes where book = ?', (book,)):
        ids.append(row[0])
        sentences.append(row[1])
    if(len(sentences) < 10): return
    # Create a feature representation of the sentences using tf-idf
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(sentences)
    # Apply K-Means clustering
    num_clusters = 9  # Change this to the number of clusters you want
    km = KMeans(n_clusters=num_clusters, n_init=10)
    km.fit(X)
    # Insert or update the cluster assignments in the database
    for i in range(len(ids)):
        # print(i)
        sentence_id = ids[i]
        cluster_id = km.labels_[i] + 1  # Add 1 to the cluster label to get cluster ID (1-based index)
        c.execute('INSERT OR REPLACE INTO clusters (id, cluster) VALUES (?, ?)',
                      (sentence_id, str(cluster_id)))

books=[ "df", "dq", "em", "fp", "ij", "mb", "rm", "ta", "td", "u"]
for book in books:
    run(book)
# Commit the changes and close the database connection
conn.commit()
conn.close()
#+end_src

#+RESULTS:
: None

#+begin_src sqlite :db ./bigother.db
select * from clusters limit 10;
#+end_src

#+RESULTS:
|  1 | 5 |
|  2 | 4 |
|  3 | 4 |
|  4 | 5 |
|  5 | 1 |
|  6 | 4 |
|  7 | 4 |
|  8 | 7 |
|  9 | 5 |
| 10 | 4 |

* scrape met
#+begin_src python :tangle scrape-met.py
#! /usr/bin/env nix-shell
#! nix-shell -i python3 -p "python3.withPackages(p: [p.tqdm p.requests])"
import time
import sqlite3
import requests
from tqdm import tqdm

# Create a SQLite database and table to store the JSON data
conn = sqlite3.connect('met_objects.db')
c = conn.cursor()
c.execute('CREATE TABLE IF NOT EXISTS MetObjects (id INT PRIMARY KEY, data TEXT)')

# Define the endpoint URL for object IDs and data
object_ids_endpoint = 'https://collectionapi.metmuseum.org/public/collection/v1/objects'
data_endpoint = 'https://collectionapi.metmuseum.org/public/collection/v1/objects/{}'

# Make a request to the object IDs endpoint to get the list of IDs
response = requests.get(object_ids_endpoint)
if response.ok:
    object_ids = response.json()['objectIDs']
else:
    # Handle an error response from the endpoint, if necessary
    print(f"Error retrieving object IDs: {response.status_code} - {response.text}")
    object_ids = []

# Set a delay of 0.025 seconds to make no more than 40 requests per second
delay = 0.025

# Iterate through the object IDs and download the data from the endpoint
with tqdm(total=len(object_ids), desc='Progress', unit='object') as pbar:
    for object_id in object_ids:
        response = requests.get(data_endpoint.format(object_id))
        if response.ok:
            data = response.json()

            # Store the data in the database
            c.execute('INSERT INTO MetObjects VALUES (?, ?)', (object_id, str(data)))
            conn.commit()

            # Wait for the specified delay between requests
            time.sleep(delay)

        # Update the progress bar
        pbar.update(1)

conn.close()
#+end_src

#+RESULTS:

* chicago art set
** chicago metadata
#+begin_src python
import csv
import sqlite3

# Connect to the SQLite database
conn = sqlite3.connect('bigother.db')

# Define the cursor object
c = conn.cursor()

# Create the "art" table if it doesn't exist
c.execute('''CREATE TABLE IF NOT EXISTS art_chicago
             (id INTEGER PRIMARY KEY,
              meta_id INTEGER,
              subpic TEXT,
              filename TEXT,
              size INTEGER,
              md5 TEXT,
              width INTEGER,
              height INTEGER,
              art_url TEXT,
              artist TEXT,
              title TEXT,
              subpic_title TEXT,
              origin TEXT,
              date TEXT,
              medium TEXT,
              tags TEXT)''')

# Open the TSV file and insert its data into the "art" table
with open('meta.txt', 'r') as tsv_file:
    reader = csv.reader(tsv_file, delimiter='\t')
    for row in reader:
        c.execute('''INSERT INTO art_chicago (meta_id, subpic, filename, size, md5, width, height, art_url, artist, title, subpic_title, origin, date, medium, tags)
                     VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''', row)

# Commit the changes to the database
conn.commit()

# Close the database connection
conn.close()
#+end_src
** add images
#+begin_src python :tangle /ssh:yui:art.py
import csv
import os
import sqlite3

# Connect to the database
conn = sqlite3.connect('imgs.db')
c = conn.cursor()

# Create the table to hold the image data
c.execute('''CREATE TABLE IF NOT EXISTS images (name text, image blob)''')

# Open the CSV file and read it line by line
with open('oilcanvas2.csv') as csvfile:
    reader = csv.reader(csvfile)
    for row in reader:
        # Get the filename (assuming it's the 4th column)
        filename = row[3]
        # Download the file using rsync
        os.system('rsync -avz -e ssh "groupchattt:familyshare/completed/Art Institute of Chicago/Art/{}" .'.format(filename))
        print("downloaded")
        # Add the image to the SQLite database
        with open(filename, 'rb') as f:
            img_data = f.read()
        c.execute("INSERT INTO images VALUES (?, ?)", (filename, sqlite3.Binary(img_data)))
        # Remove the image file we just added to the database.
        os.remove(filename)

# Commit the changes and close the database connection
conn.commit()
conn.close()
#+end_src
