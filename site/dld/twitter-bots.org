#+TITLE: Two Twitter bots for your consideration

* Trash factories
Here at joegame we have our hands in a couple of jars.  In particular the figure of the bot looms large in our aesthetic milieu.  The Twitter bot is, in particular, a curious medium: it accomplishes more than most in being /anti/-art, while still being recognizable as an art type product.  The kinds of bots we make here [fn:1] are in many ways /factories/.  We believe the figure of the factory is a good candidate for an anodyne source of culture.  Why?  Because it runs counter to almost every single tendency existing which poses as an important vehicle, or minimally a concern, for culture.  Where right now so many artists fight for a certain kind entitlement granted by authorship, where new bobbles and coins come in to assure that /you who drew this picture/ can be properly compensated in a market that has otherwise abandoned you, the twitter bots we make strive to break from a model where there is an /artist/ whose individual encapsulations of talent and patience may be rewarded, and in that, counter the idea that the only problems are those of /allocation/, that of course /you deserve/ this and that.  To say that the only problem is one of allocation is to admit a certain kind of defeat: it is to keep playing a game.

Our twitter bots are not playing the same old game, they are instead /trash factories/.  They don't aim to be good, the aim to just exist, to play out a pattern with very little value.  They are not /for/ you.  They are more like the emails in a spam folder than they are like any kind of art you know of.

* Twitter dreams
#+caption: /On the interpretation of Dreams/ pg. 297
#+begin_quote
There is an answer, which at first sight seems most plausible, to the argument that the great lack of proportion between the dream-content and the dream-thoughts implies that the psychical material has undergone an extensive process of condensation in the course of the formation of the dream. We very often have an impression that we have dreamt a great deal all through the night and have since forgotten most of what we dreamt. On this view, the dream which we remember when we wake up would only be a fragmentary remnant of the total dream-work; and this, if we could recollect it in its entirety, might well be as extensive as the dreamthoughts. There is undoubtedly some truth in this: there can be no question that dreams can be reproduced most accurately if we try to recall them as soon as we wake up and that our memory of them becomes more and more incomplete towards evening. But on the other hand it can be shown that the impression that we have dreamt a great deal more than we can reproduce is very often based on an illusion...
#+end_quote

The twitter dreams bot is fairly simple: every hour, it taps into a Twitter API endpoint and tries to collect a certain number of /unique/ and /sfw/[fn:2] images being posted at the moment.  Once it has enough pictures downloaded, it goes through another procedure which takes each image and overlays it over the previous image, after procedurally adding a lot of transparency.  In parallel, we create our music for the movie.  The final step takes the processed images and the generated soundtrack, and makes a movie to upload to twitter.

Here are the steps.
** Twitter API scraping
The endpoint this bot uses is from the new "v2" edition of the Twitter API.  If you have the right secret codes[fn:3] you can point =curl= command to this URL and get "1% of all Tweets in real-time": https://api.twitter.com/2/tweets/sample/stream.  It is a wonderful endpoint, in part because I can't actually think of another way for anyone to get outside the algorithm, or even outside our "echo chambers"[fn:4].  When you open up the stream, the velocity of %1 of real-time tweets is extremely fast, and it doesn't slow down.

In this, there is an interesting problem to solve concerning two different asynchronous processes: there is the asynchronous barrage of tweets, and then there is the completely different of downloading and validating and writing those images.  There is no way (that we can see) to pre-filter the sampled stream, so the tweets that come through the endpoint are not guaranteed to have images; but even after filtering out for images, we need to decide whether this picture is unique; and /then/ we need detect if its porn or not.  There is nothing actually tying these processes together as /dependent/ upon one another other than /fidelity to the vision/.

What we mean is that we want as much as possible to retain the sense that these are realtime slices of Twitter.  One naive way of solving the problem would be just to go one at a time: open the stream, acquire a tweet and close the stream, check for all three conditions (has image, has unique image, has unique non-porn image), download image, and then repeat.[fn:5]  But this feels like a waste of the "stream" part of the endpoint, and just /feels/ clunky, like turning on the tap at each separate time we're ready for another drop.

But we can't rely completely on /another/ naive strategy, which is to rely on the stream's EventEmitter and some kind of global count.  When I did this before, I found that some kind of, I presume, race condition was causing many potential pictures to get downloaded before they were verified, and the program itself was having a hard time /stopping/ itself once it had reached its necessary quota.  This is probably something many real engineers have to deal with when you are working with a lot of scale.  It makes it fun for us as wannabes to have to deal with this.

The strategy we ended up with is this: the stream emits events when a tweet arrives; within the logic of handling the event we make the first check, that there is an image attached to this tweet:
#+begin_src typescript
// method within the PictureScraper class
  dataCb(data: any) {
    if (data.includes?.media) {
      const media = data.includes?.media.filter((item: {
          type: string,
          url: string,
          media_key: string }) => {
        return item.type == 'photo'
      })
      if (media.length > 0) {
        const randI = Math.floor(Math.random() * media.length) // if there are multiple pictures, pick one
        this.addImgUrl(media[randI].url) // entering the queue
      }
    }
  }
#+end_src
This is a callback function to a certain event, it has one possible side effect which is adding the url of an image to add an image url to an queue that is just an array within the class:
#+begin_src typescript
  addImgUrl(url){
    if(this.imgUrls.length >= this.maxImgUrlStack ){
      this.imgUrls.shift()
    }
    this.imgUrls.push(url)
  }
#+end_src
Note here the shift and push.  Even with a queue like this set up, there is no point in it being just an unlimitedly large stack, or at least, it isn't better than our first naive idea.  The rate at which image urls are collected is fast, and sometimes something will timeout; it makes me uneasy leaving such a fast moving stream with total freedom to grow the stack, even if it is just an array of strings.  So it is decided that the list of urls to-be-processed can only grow so large, but it doesn't simply throw away images if the stack has reached its limit, rather, it drops the oldest one in the stack, and adds the new one in again at front.  This way, the queue itself is "fresh," more contemporaneous with the process.  The last bit needed here is the pop.  We will get to that.

Delaying talking about /when/ the image is downloaded, what about the uniqueness condition?  There are some pretty sophisticated algorithms to choose from in this space, concerning the "distance" between images, where if two images are the similar, they have closer distance.  But our needs are simple, because we are not looking for /similar/ images, but the same image, presumably uploaded a million times but some botnet or another.  In this, the Jimp library provides a handy perceptual hash method to image objects within its framework, which does what it sounds like and essentially "reduces" images to a small-ish string of characters.  With this, it is trivial to search for duplicates, you simply do not replace a hash we already have!
#+begin_src typescript
  async getImg(url: string): Promise<[Jimp, string] | undefined> {
    let img: Jimp
    let hash: string
    try {
      img = await downloadJimpImg(url)
      hash = img.hash()

    } catch (err) {
      // Be eager, move on
      return undefined
    }
    if (!(this.hashes.some(h => h == hash))) {
        // If it is not the case that this hash we are bringing is found in the array,
        // then this image is one we can keep. (Assuming not porn)
      return [img, hash]
    }
    return undefined
  }
#+end_src
We have code to process the stream, and code to download an image, so we simply need something to bring it all together, to /maybe/ get an image.
#+begin_src typescript
  async maybeGetImg() {
    if (this.imgUrls.length < 1) return
    if (this.currentlyDownloading >= this.maxDownloadStack) return
    this.currentlyDownloading += 1
    const img = await this.getImg(this.imgUrls.pop()) // Pop!
    if (img) {
      const nres = await this.checkNSFW(img[0])
      if (this.extractNeutralProb(nres) > 0.92) {
        await this.writeFile(img)
        console.log(this.i + "downloaded")
        if (this.i > this.amount) {
          console.log('done')
          this.stream.close();
          process.exit()
        }
      }
    }
    this.currentlyDownloading -= 1
  }
#+end_src
This callback also handles deciding when we are done, and closing the !
Here is the rest of the work.  One more "stack" which is just a counter and controls how many extra await threads to create in its work.

* Footnotes

[fn:1] Other than, of course, the official Joegame bot, which we will talk about later, when we talk about the desert.

[fn:2] The original iteration of the bot lacked the assurances of being "safe for work," and there are still quite a lot of arguments in our offices about this.  On the one hand, filtering out /anything/ (that is unique) is against the dream model we are playing with here: dreams are /the/ space where anything goes, that is what makes them what they are.  On the other hand, as Freud lays out, the very nature of dreams /as they are experienced/ are what they are because of repression, and so we can allow ourselves a little of our own repression in the form of ML model.  But then again: that would imply that somethings status as pornography is alone a basis for its repression, which it is not.  We made the final decision once we had been given enough warnings from Twitter's /own/ ML model. TODO add picture.

[fn:3] Luckily, if you just want to sample like we are this point, it does not take too much to get the right codes.  Only once you want to start posting things outside of an official app of the website do things become a little gated off

[fn:4] The concept of an "echo chamber" is probably at this point a "dog whistle," proposing that, in fact, the things we all come together to agree on are tainted because, presumably, there was someone we didn't consult.  The organic intellectual will come from an echo chamber.

[fn:5] We might pause here to wonder if, at the end of the day, anything is lost with this approach vs. what was actually implemented.  There is probably little difference, the "time" of the dreams would be roughly comparable, and I can't think of an limitations on the API side as long as the streams are closed and opened correctly.  It would mean that the entire process could be a simple function, and there would be less RAM sacrificed.  We can only say this: it is less pretty; it is not really using the stream.  It is distinguished in the same way as differing strategies of washing dishes.  Some people are good about turning the water on and off, some people are good at keeping the water on, but not letting it go to waste.
